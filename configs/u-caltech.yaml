# Configuration file for u-caltech dataset

data_dir: "../data/u-caltech"
val_split_ratio: 0.2  # 20% of training set for validation

# Data loader settings
data_loader:
  num_workers: null  # null = auto-detect (4 for GPU, 2 for CPU)
  pin_memory: null  # null = auto-detect (True for CUDA, False otherwise)

# Coarse Reconstruction Configuration
coarse:
  # Model parameters
  model:
    time_steps: 25
    v_threshold: 1.0
    tau: 2.0
    in_channels: 1
    out_channels: 3
  
  # Loss parameters
  # Simplified loss: Focus on basic reconstruction first (L1+L2)
  # Perceptual/semantic losses may be dominating and preventing learning
  loss:
    type: "reconstruction"
    l1_weight: 1.0
    l2_weight: 1.0
    perceptual_weight: 0.0  # Disabled temporarily - was preventing basic learning
    semantic_weight: 0.0  # Disabled temporarily - was preventing basic learning
    gradient_weight: 0.0  # Disabled temporarily - focus on basic reconstruction first
  
  # Training parameters
  training:
    epochs: 50  # Increased significantly to allow proper convergence
    batch_size: 8
    learning_rate: 0.0001  # Increased from 4e-5 to 1e-4 for better learning signal
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 15  # Increased significantly to prevent premature stopping
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adam"  # According to paper: Adam (not AdamW)
    lr: 0.0001  # Increased to match training LR for better learning
    weight_decay: 0.0  # Paper doesn't mention weight decay (set to 0)
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "plateau"  # Plateau scheduler - reduces LR only when loss plateaus
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
    patience: 5  # Increased from 3 to 5 - reduce LR after 5 epochs without improvement
    factor: 0.5  # For plateau: multiply LR by 0.5 when reducing
    min_lr: 0.00001  # Increased minimum LR from 1e-6 to 1e-5 to prevent too low learning
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Prompt Learning Configuration
prompt:
  # Model parameters - HQ/LQ Prompt Adapter
  model:
    clip_model_name: "ViT-B/32"  # CLIP model name
    prompt_dim: 77  # Prompt dimension
    freeze_image_encoder: true  # Freeze CLIP image encoder
    hq_generation_method: "mixture"  # HQ image generation method: "mixture" (default), "tfi", "wgse", or "weighted_avg"
  
  # Loss parameters - HQ/LQ Prompt Loss (binary classification)
  loss:
    type: "hq_lq_prompt"  # Use HQ/LQ prompt loss (binary classification)
    temperature: 0.07  # Temperature for contrastive loss
  
  # Training parameters
  training:
    epochs: 10  # Increased from 5 to allow more training for prompts to learn
    batch_size: 8
    learning_rate: 0.00004  # According to paper: 4e-5 for U-CALTECH
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 8  # Increased from 2 to allow more training before stopping
    early_stopping_min_delta: 0.000001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.00004  # According to paper: 4e-5 for U-CALTECH
    weight_decay: 0.0001  # Reduced from 0.01 to allow better learning
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "constant"  # Changed from cosine to constant - with only 1 epoch, cosine scheduler decays LR to 0 immediately
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Refinement Configuration
refine:
  # Model parameters
  model:
    in_channels: 3
    out_channels: 3
    base_channels: 64
    num_down: 4  # Number of downsampling layers
  
  # Loss parameters - FIX: Reconstruction-focused loss (not prompt-dominated)
  # Prompt loss was causing abstract feature maps instead of refined images
  loss:
    type: "prompt"  # Prompt loss (but reduced weight)
    temperature: 0.07  # Temperature for contrastive loss
    prompt_weight: 0.1  # FIX: Reduced from 10.0 - was dominating and causing abstract outputs
    # Primary loss is now reconstruction (L1 + L2), prompt is secondary guide
    reconstruction_weight: 1.0  # Primary: Reconstruction loss (L1)
    structure_weight: 0.1  # FIX: Reduced from 0.5 - was preventing sharpening
    l2_weight: 0.05  # FIX: Reduced from 0.1 - was causing blur
    perceptual_weight: 0.0  # Disabled - was causing issues
  
  # Training parameters
  training:
    epochs: 20  # According to paper: 19 epochs for Stage 3 (U-CALTECH)
    batch_size: 8
    learning_rate: 0.00004  # According to paper: 4e-5 for U-CALTECH
    seed: 42
    use_amp: false
    grad_clip: 1.0  # Gradient clipping to prevent explosion with high prompt_weight=100.0
    early_stopping_patience: 10  # Early stopping will stop training if no improvement for 10 epochs
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.00004  # According to paper: 4e-5 for U-CALTECH
    weight_decay: 0.0001
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

labels:
  - accordion
  - airplanes
  - anchor
  - ant
  - barrel
  - bass
  - beaver
  - binocular
  - bonsai
  - brain
  - brontosaurus
  - buddha
  - butterfly
  - camera
  - cannon
  - car
  - ceilingfan
  - cellphone
  - chair
  - chandelier
  - cougarbody
  - cougarface
  - crab
  - crayfish
  - crocodile
  - crocodilehead
  - cup
  - dalmatian
  - dollarbill
  - dolphin
  - dragonfly
  - electricguitar
  - elephant
  - emu
  - euphonium
  - ewer
  - faces
  - ferry
  - flamingo
  - flamingohead
  - garfield
  - gerenuk
  - gramophone
  - grandpiano
  - hawksbill
  - headphone
  - hedgehog
  - helicopter
  - ibis
  - inlineskate
  - joshuatree
  - kangaroo
  - ketch
  - lamp
  - laptop
  - Leopards
  - llama
  - lobster
  - lotus
  - mandolin
  - mayfly
  - menorah
  - metronome
  - minaret
  - Motorbikes
  - nautilus
  - octopus
  - okapi
  - pagoda
  - panda
  - pigeon
  - pizza
  - platypus
  - pyramid
  - revolver
  - rhino
  - rooster
  - saxophone
  - schooner
  - scissors
  - scorpion
  - seahorse
  - snoopy
  - soccerball
  - stapler
  - starfish
  - stegosaurus
  - stopsign
  - strawberry
  - sunflower
  - tick
  - trilobite
  - umbrella
  - watch
  - waterlilly
  - wheelchair
  - wildcat
  - windsorchair
  - wrench
  - yinyang
  - background

