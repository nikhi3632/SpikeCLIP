# Configuration file for u-caltech dataset

data_dir: "../data/u-caltech"
val_split_ratio: 0.2  # 20% of training set for validation

# Data loader settings
data_loader:
  num_workers: null  # null = auto-detect (4 for GPU, 2 for CPU)
  pin_memory: null  # null = auto-detect (True for CUDA, False otherwise)

# Coarse Reconstruction Configuration
coarse:
  # Model parameters
  model:
    time_steps: 25
    v_threshold: 1.0
    tau: 2.0
    in_channels: 1
    out_channels: 3
  
  # Loss parameters
  loss:
    type: "reconstruction"
    l1_weight: 1.0
    l2_weight: 1.0
    perceptual_weight: 1.0  # Increased significantly to preserve structure (SSIM improvement)
    semantic_weight: 2.0  # Increased further for better semantic alignment (critical for classification)
  
  # Training parameters
  training:
    epochs: 5  # According to paper: 5 epochs for Stage 1 (U-CALTECH)
    batch_size: 8
    learning_rate: 0.00004  # According to paper: 4e-5 for U-CALTECH
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 10  # Early stopping will stop training if no improvement for 10 epochs
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better regularization
    lr: 0.00004  # According to paper: 4e-5 for U-CALTECH
    weight_decay: 0.0001  # Added weight decay to prevent collapse
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Prompt Learning Configuration
prompt:
  # Model parameters - HQ/LQ Prompt Adapter
  model:
    clip_model_name: "ViT-B/32"  # CLIP model name
    prompt_dim: 77  # Prompt dimension
    freeze_image_encoder: true  # Freeze CLIP image encoder
    hq_generation_method: "mixture"  # HQ image generation method: "mixture" (default), "tfi", "wgse", or "weighted_avg"
  
  # Loss parameters - HQ/LQ Prompt Loss (binary classification)
  loss:
    type: "hq_lq_prompt"  # Use HQ/LQ prompt loss (binary classification)
    temperature: 0.07  # Temperature for contrastive loss
    multi_class_weight: 3.0  # Weight for multi-class classification loss (increased to improve multi-class accuracy)
  
  # Training parameters
  training:
    epochs: 1  # According to paper: 1 epoch for Stage 2 (U-CALTECH)
    batch_size: 8
    learning_rate: 0.00004  # According to paper: 4e-5 for U-CALTECH
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 10  # Early stopping will stop training if no improvement for 10 epochs
    early_stopping_min_delta: 0.000001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.00004  # According to paper: 4e-5 for U-CALTECH
    weight_decay: 0.0001  # Reduced from 0.01 to allow better learning
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Refinement Configuration
refine:
  # Model parameters
  model:
    in_channels: 3
    out_channels: 3
    base_channels: 64
    num_down: 4  # Number of downsampling layers
  
  # Loss parameters - Prompt Loss + InfoNCE Loss (according to paper)
  # According to paper: L_total = L_class + λ*L_prompt (λ=100)
  loss:
    type: "prompt"  # Dummy type (not used, we use prompt_loss + class_loss)
    temperature: 0.07  # Temperature for contrastive loss
    prompt_weight: 100.0  # λ=100 according to paper (strong HQ alignment emphasis)
    class_loss_weight: 1.0  # α=1.0 according to paper (L_class has weight 1.0)
    # Note: Paper only uses L_class + λ*L_prompt, no additional losses
    # We keep identity_penalty, structure_weight, perceptual_weight for stability
    # but they should be set to 0.0 to match paper exactly
    identity_penalty: 0.0  # Paper doesn't use this (set to 0.0 to match paper)
    structure_weight: 0.0  # Paper doesn't use this (set to 0.0 to match paper)
    perceptual_weight: 0.0  # Paper doesn't use this (set to 0.0 to match paper)
  
  # Training parameters
  training:
    epochs: 19  # According to paper: 19 epochs for Stage 3 (U-CALTECH)
    batch_size: 8
    learning_rate: 0.00004  # According to paper: 4e-5 for U-CALTECH
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 10  # Early stopping will stop training if no improvement for 10 epochs
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.00004  # According to paper: 4e-5 for U-CALTECH
    weight_decay: 0.0001
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

labels:
  - accordion
  - airplanes
  - anchor
  - ant
  - barrel
  - bass
  - beaver
  - binocular
  - bonsai
  - brain
  - brontosaurus
  - buddha
  - butterfly
  - camera
  - cannon
  - car
  - ceilingfan
  - cellphone
  - chair
  - chandelier
  - cougarbody
  - cougarface
  - crab
  - crayfish
  - crocodile
  - crocodilehead
  - cup
  - dalmatian
  - dollarbill
  - dolphin
  - dragonfly
  - electricguitar
  - elephant
  - emu
  - euphonium
  - ewer
  - faces
  - ferry
  - flamingo
  - flamingohead
  - garfield
  - gerenuk
  - gramophone
  - grandpiano
  - hawksbill
  - headphone
  - hedgehog
  - helicopter
  - ibis
  - inlineskate
  - joshuatree
  - kangaroo
  - ketch
  - lamp
  - laptop
  - Leopards
  - llama
  - lobster
  - lotus
  - mandolin
  - mayfly
  - menorah
  - metronome
  - minaret
  - Motorbikes
  - nautilus
  - octopus
  - okapi
  - pagoda
  - panda
  - pigeon
  - pizza
  - platypus
  - pyramid
  - revolver
  - rhino
  - rooster
  - saxophone
  - schooner
  - scissors
  - scorpion
  - seahorse
  - snoopy
  - soccerball
  - stapler
  - starfish
  - stegosaurus
  - stopsign
  - strawberry
  - sunflower
  - tick
  - trilobite
  - umbrella
  - watch
  - waterlilly
  - wheelchair
  - wildcat
  - windsorchair
  - wrench
  - yinyang
  - background

