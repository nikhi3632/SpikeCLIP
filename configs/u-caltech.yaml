# Configuration file for u-caltech dataset

data_dir: "../data/u-caltech"
val_split_ratio: 0.2  # 20% of training set for validation

# Data loader settings
data_loader:
  num_workers: null  # null = auto-detect (4 for GPU, 2 for CPU)
  pin_memory: null  # null = auto-detect (True for CUDA, False otherwise)

# Coarse Reconstruction Configuration
coarse:
  # Model parameters
  model:
    time_steps: 25
    v_threshold: 1.0
    tau: 2.0
    in_channels: 1
    out_channels: 3
  
  # Loss parameters
  loss:
    type: "reconstruction"
    l1_weight: 1.0
    l2_weight: 1.0
    perceptual_weight: 0.0  # According to paper: only L1/L2 loss (no perceptual)
    semantic_weight: 0.0  # According to paper: only L1/L2 loss (no semantic)
    gradient_weight: 0.3  # Increased from 0.1 to preserve sharp edges and details
  
  # Training parameters
  training:
    epochs: 30  # Increased to allow more learning (with increased patience, model can train longer)
    batch_size: 8
    learning_rate: 0.00006  # Slightly increased from 4e-5 to 6e-5 for better learning
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 10  # Increased from 5 to 10 to allow more training time
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adam"  # According to paper: Adam (not AdamW)
    lr: 0.00006  # Slightly increased from 4e-5 to 6e-5 to match training LR
    weight_decay: 0.0  # Paper doesn't mention weight decay (set to 0)
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "plateau"  # Changed from cosine to plateau - reduces LR only when loss plateaus (prevents premature decay)
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
    patience: 3  # For plateau: reduce LR after 3 epochs without improvement
    factor: 0.5  # For plateau: multiply LR by 0.5 when reducing
    min_lr: 0.000001  # For plateau: minimum learning rate (prevent zero LR) - use decimal format for YAML
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Prompt Learning Configuration
prompt:
  # Model parameters - HQ/LQ Prompt Adapter
  model:
    clip_model_name: "ViT-B/32"  # CLIP model name
    prompt_dim: 77  # Prompt dimension
    freeze_image_encoder: true  # Freeze CLIP image encoder
    hq_generation_method: "mixture"  # HQ image generation method: "mixture" (default), "tfi", "wgse", or "weighted_avg"
  
  # Loss parameters - HQ/LQ Prompt Loss (binary classification)
  loss:
    type: "hq_lq_prompt"  # Use HQ/LQ prompt loss (binary classification)
    temperature: 0.07  # Temperature for contrastive loss
    multi_class_weight: 8.0  # Increased from 3.0 to 8.0 for stronger multi-class learning
  
  # Training parameters
  training:
    epochs: 10  # Increased from 5 to allow more training for prompts to learn
    batch_size: 8
    learning_rate: 0.00004  # According to paper: 4e-5 for U-CALTECH
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 8  # Increased from 2 to allow more training before stopping
    early_stopping_min_delta: 0.000001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.00004  # According to paper: 4e-5 for U-CALTECH
    weight_decay: 0.0001  # Reduced from 0.01 to allow better learning
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "constant"  # Changed from cosine to constant - with only 1 epoch, cosine scheduler decays LR to 0 immediately
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Refinement Configuration
refine:
  # Model parameters
  model:
    in_channels: 3
    out_channels: 3
    base_channels: 64
    num_down: 4  # Number of downsampling layers
  
  # Loss parameters - Prompt Loss + InfoNCE Loss (according to paper)
  # According to paper: L_total = L_class + λ*L_prompt (λ=100)
  loss:
    type: "prompt"  # Dummy type (not used, we use prompt_loss + class_loss)
    temperature: 0.07  # Temperature for contrastive loss
    prompt_weight: 10.0  # Reduced from 100.0 for training stability (can increase if stable)
    class_loss_weight: 2.5  # Increased from 1.0 to 2.5 to emphasize classification accuracy
    # Note: Paper uses λ=100, but we start with 10.0 for stability
    # Additional losses for stability (prevent image degradation):
    identity_penalty: 1.0  # Prevent identity mapping and image degradation
    structure_weight: 0.5  # Preserve image structure (prevent negative SSIM)
    perceptual_weight: 0.5  # Preserve perceptual quality
  
  # Training parameters
  training:
    epochs: 19  # According to paper: 19 epochs for Stage 3 (U-CALTECH)
    batch_size: 8
    learning_rate: 0.00004  # According to paper: 4e-5 for U-CALTECH
    seed: 42
    use_amp: false
    grad_clip: 1.0  # Gradient clipping to prevent explosion with high prompt_weight=100.0
    early_stopping_patience: 10  # Early stopping will stop training if no improvement for 10 epochs
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.00004  # According to paper: 4e-5 for U-CALTECH
    weight_decay: 0.0001
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

labels:
  - accordion
  - airplanes
  - anchor
  - ant
  - barrel
  - bass
  - beaver
  - binocular
  - bonsai
  - brain
  - brontosaurus
  - buddha
  - butterfly
  - camera
  - cannon
  - car
  - ceilingfan
  - cellphone
  - chair
  - chandelier
  - cougarbody
  - cougarface
  - crab
  - crayfish
  - crocodile
  - crocodilehead
  - cup
  - dalmatian
  - dollarbill
  - dolphin
  - dragonfly
  - electricguitar
  - elephant
  - emu
  - euphonium
  - ewer
  - faces
  - ferry
  - flamingo
  - flamingohead
  - garfield
  - gerenuk
  - gramophone
  - grandpiano
  - hawksbill
  - headphone
  - hedgehog
  - helicopter
  - ibis
  - inlineskate
  - joshuatree
  - kangaroo
  - ketch
  - lamp
  - laptop
  - Leopards
  - llama
  - lobster
  - lotus
  - mandolin
  - mayfly
  - menorah
  - metronome
  - minaret
  - Motorbikes
  - nautilus
  - octopus
  - okapi
  - pagoda
  - panda
  - pigeon
  - pizza
  - platypus
  - pyramid
  - revolver
  - rhino
  - rooster
  - saxophone
  - schooner
  - scissors
  - scorpion
  - seahorse
  - snoopy
  - soccerball
  - stapler
  - starfish
  - stegosaurus
  - stopsign
  - strawberry
  - sunflower
  - tick
  - trilobite
  - umbrella
  - watch
  - waterlilly
  - wheelchair
  - wildcat
  - windsorchair
  - wrench
  - yinyang
  - background

