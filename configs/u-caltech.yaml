# Configuration file for u-caltech dataset

data_dir: "../data/u-caltech"
val_split_ratio: 0.2  # 20% of training set for validation

# Data loader settings
data_loader:
  num_workers: null  # null = auto-detect (4 for GPU, 2 for CPU)
  pin_memory: null  # null = auto-detect (True for CUDA, False otherwise)

# Coarse Reconstruction Configuration
coarse:
  # Model parameters
  model:
    time_steps: 25
    v_threshold: 1.0
    tau: 2.0
    in_channels: 1
    out_channels: 3
  
  # Loss parameters
  loss:
    type: "reconstruction"
    l1_weight: 1.0
    l2_weight: 1.0
    perceptual_weight: 0.5  # Balanced: enough for semantic learning, not too much to cause blurring
    semantic_weight: 1.5  # Increased for better semantic alignment (critical for classification)
  
  # Training parameters
  training:
    epochs: 100  # Increased from 30 for better convergence and semantic learning
    batch_size: 8
    learning_rate: 0.0002  # Increased from 0.0001 for faster learning while avoiding collapse
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 5  # Stop if no improvement for 5 epochs
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better regularization
    lr: 0.0002  # Can override training.learning_rate (increased from 0.0001)
    weight_decay: 0.0001  # Added weight decay to prevent collapse
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Prompt Learning Configuration
prompt:
  # Model parameters - HQ/LQ Prompt Adapter
  model:
    clip_model_name: "ViT-B/32"  # CLIP model name
    prompt_dim: 77  # Prompt dimension
    freeze_image_encoder: true  # Freeze CLIP image encoder
    hq_generation_method: "mixture"  # HQ image generation method: "mixture" (default), "tfi", "wgse", or "weighted_avg"
  
  # Loss parameters - HQ/LQ Prompt Loss (binary classification)
  loss:
    type: "hq_lq_prompt"  # Use HQ/LQ prompt loss (binary classification)
    temperature: 0.07  # Temperature for contrastive loss
    multi_class_weight: 2.0  # Weight for multi-class classification loss (balanced: not too high to avoid overfitting)
  
  # Training parameters
  training:
    epochs: 150  # Increased significantly to allow more time for learning discriminative features (target: 80%+ accuracy)
    batch_size: 8
    learning_rate: 0.001  # Increased from 0.0005 for faster learning with higher multi_class_weight
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 10  # Increased to allow more learning time
    early_stopping_min_delta: 0.000001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.001  # Can override training.learning_rate (match training.learning_rate for consistency)
    weight_decay: 0.0001  # Reduced from 0.01 to allow better learning
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

# Refinement Configuration
refine:
  # Model parameters
  model:
    in_channels: 3
    out_channels: 3
    base_channels: 64
    num_down: 4  # Number of downsampling layers
  
  # Loss parameters - Prompt Loss + InfoNCE Loss (according to paper)
  loss:
    type: "prompt"  # Dummy type (not used, we use prompt_loss + class_loss)
    temperature: 0.07  # Temperature for contrastive loss
    prompt_weight: 0.5  # λ=0.5 (reduced to prevent over-emphasis on HQ alignment)
    class_loss_weight: 1.5  # α=1.5 (balanced: enough for classification, not too high to degrade quality)
    identity_penalty: 0.5  # Penalty for identity mapping (increased to prevent copying)
    structure_weight: 2.0  # Weight for structure preservation (increased to preserve image quality)
    perceptual_weight: 1.5  # Weight for perceptual loss (balanced: semantic improvement without degradation)
  
  # Training parameters
  training:
    epochs: 100  # Increased significantly to allow more learning for better classification (target: 80%+ accuracy)
    batch_size: 8
    learning_rate: 0.0005  # Increased from 0.0001 for faster learning with higher class_loss_weight
    seed: 42
    use_amp: false
    grad_clip: 1.0
    early_stopping_patience: 10  # Increased to allow more learning time
    early_stopping_min_delta: 0.0001  # Minimum improvement to reset patience
  
  # Optimizer configuration
  optimizer:
    type: "adamw"  # Changed to AdamW for better generalization
    lr: 0.0005  # Can override training.learning_rate (match training.learning_rate for consistency)
    weight_decay: 0.0001
    momentum: 0.9  # Only for SGD
  
  # Scheduler configuration
  scheduler:
    type: "cosine"  # cosine, step, plateau
    step_size: 30  # Only for step scheduler
    gamma: 0.1  # Only for step scheduler
  
  # Output paths
  output:
    checkpoint_dir: "outputs/checkpoints/ucaltech"
    log_dir: "outputs/logs"

labels:
  - accordion
  - airplanes
  - anchor
  - ant
  - barrel
  - bass
  - beaver
  - binocular
  - bonsai
  - brain
  - brontosaurus
  - buddha
  - butterfly
  - camera
  - cannon
  - car
  - ceilingfan
  - cellphone
  - chair
  - chandelier
  - cougarbody
  - cougarface
  - crab
  - crayfish
  - crocodile
  - crocodilehead
  - cup
  - dalmatian
  - dollarbill
  - dolphin
  - dragonfly
  - electricguitar
  - elephant
  - emu
  - euphonium
  - ewer
  - faces
  - ferry
  - flamingo
  - flamingohead
  - garfield
  - gerenuk
  - gramophone
  - grandpiano
  - hawksbill
  - headphone
  - hedgehog
  - helicopter
  - ibis
  - inlineskate
  - joshuatree
  - kangaroo
  - ketch
  - lamp
  - laptop
  - Leopards
  - llama
  - lobster
  - lotus
  - mandolin
  - mayfly
  - menorah
  - metronome
  - minaret
  - Motorbikes
  - nautilus
  - octopus
  - okapi
  - pagoda
  - panda
  - pigeon
  - pizza
  - platypus
  - pyramid
  - revolver
  - rhino
  - rooster
  - saxophone
  - schooner
  - scissors
  - scorpion
  - seahorse
  - snoopy
  - soccerball
  - stapler
  - starfish
  - stegosaurus
  - stopsign
  - strawberry
  - sunflower
  - tick
  - trilobite
  - umbrella
  - watch
  - waterlilly
  - wheelchair
  - wildcat
  - windsorchair
  - wrench
  - yinyang
  - background

